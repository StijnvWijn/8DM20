{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MNIST digit recognition with LeNet</h1>\n",
    "\n",
    "In this practical session we will build a convolutional neural network that is able to recognise the digits 0-9 in images.\n",
    "\n",
    "You can run the code in a cell by selecting the cell and pressing Shift+Enter.\n",
    "\n",
    "<h4>1) Import statements</h4>\n",
    "First, import some of the packages we will need (run the cell below).\n",
    "\n",
    "Documentation for each of these packages can be found online: <br>\n",
    "For numpy: https://docs.scipy.org/doc/numpy-dev/user/quickstart.html <br>\n",
    "For matplotlib: http://matplotlib.org/api/pyplot_api.html <br>\n",
    "For lasagne: http://lasagne.readthedocs.io <br>\n",
    "For random: https://docs.python.org/2/library/random.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import theano\n",
    "import lasagne\n",
    "import time\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2) Loading the data</h4>\n",
    "Download the data from: http://deeplearning.net/data/mnist/mnist.pkl.gz and save it somewhere on your disc. The function below loads the data from the location where you have saved it (path) and stores it in numpy arrays. The data is already split in a train set, a validation set and a test set. Each of these three sets are saved in two separate variables, one containing the labels and one containing the images. The labels are lists of numbers between 0 and 9. The images are 4-dimensional arrays (of the same length) with the image dimensions in the last 2 dimensions.\n",
    "\n",
    "Change the path in the second cell below to the location where you have saved it and run the two cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadMNIST(path):\n",
    "    f = gzip.open(path, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    train_set_labels = train_set[1]\n",
    "    train_set_images = np.resize(train_set[0],(len(train_set_labels),1,28,28))\n",
    "    train_set_images = np.pad(train_set_images,((0,0),(0,0),(2,2),(2,2)),'constant', constant_values=0)\n",
    "   \n",
    "    valid_set_labels = valid_set[1]\n",
    "    valid_set_images = np.resize(valid_set[0],(len(valid_set_labels),1,28,28))\n",
    "    valid_set_images = np.pad(valid_set_images,((0,0),(0,0),(2,2),(2,2)),'constant', constant_values=0)\n",
    "\n",
    "    test_set_labels = test_set[1]\n",
    "    test_set_images = np.resize(test_set[0],(len(test_set_labels),1,28,28))\n",
    "    test_set_images = np.pad(test_set_images,((0,0),(0,0),(2,2),(2,2)),'constant', constant_values=0)\n",
    "    \n",
    "    return train_set_labels, train_set_images, valid_set_labels, valid_set_images, test_set_labels, test_set_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_labels, train_set_images, valid_set_labels, valid_set_images, test_set_labels, test_set_images = loadMNIST(r'D:\\mnist\\mnist.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>3) Visualising the data</h4>\n",
    "Let's look at the data we've just loaded! \n",
    "\n",
    "How many samples are in each set? (Use .shape to see the dimensions) \n",
    "\n",
    "How large are the images? \n",
    "\n",
    "How many samples are there for each of the 10 digits? \n",
    "\n",
    "Show some of the images with plt.imshow (use cmap='gray_r' for black digits on a white background and interpolation='none' to see the real pixels), you can access one of the training images as: train_set_images[i,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.imshow(train_set_images[0,0],cmap='gray_r',interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>4) One-hot-encoding</h4>\n",
    "Convert the labels from a number between 0 and 9 to 'one-hot-encoding'. This means that for a label with number 3, there should be a 1 at element 3 and 0 everywhere else, i.e. [0, 0, 0, 1, 0, 0, 0, 0 ,0 ,0]. These are our target nodes, the node at position 3 should be active when the input image shows a 3. The code below does this for the training labels. \n",
    "\n",
    "Do the same for the validation labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = np.zeros((len(train_set_labels),10),dtype=np.int16)        \n",
    "for n in xrange(10):\n",
    "    labels[:,n] = train_set_labels==n\n",
    "\n",
    "print train_set_labels[:10]\n",
    "print labels[:10]\n",
    "\n",
    "#validlabels = np.zeros((len(valid_set_labels),10),dtype=np.int16)        \n",
    "#for n in xrange(10):\n",
    "#    validlabels[:,n] = valid_set_labels==n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>5) Building the network</h4>\n",
    "The function below builds the LeNet network as we looked at in the lecture. The layers are defined as functions with as input the previous network and as output the new network with the new layer added. The print network.output_shape statement shows the dimensions after the current layer. \n",
    "\n",
    "Can you recognise all the elements of the network from the lecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildLeNet(X1):\n",
    "    inputlayer = lasagne.layers.InputLayer(shape=(None, 1, 32, 32),input_var=X1)    \n",
    "    print inputlayer.output_shape\n",
    "    \n",
    "    layer1 = lasagne.layers.Conv2DLayer(inputlayer, num_filters=6, filter_size=(5,5), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    print layer1.output_shape \n",
    "    \n",
    "    layer2 = lasagne.layers.MaxPool2DLayer(layer1, pool_size=(2, 2))\n",
    "    print layer2.output_shape \n",
    "    \n",
    "    layer3 = lasagne.layers.Conv2DLayer(layer2, num_filters=16, filter_size=(5,5), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    print layer3.output_shape \n",
    "    \n",
    "    layer4 = lasagne.layers.MaxPool2DLayer(layer3, pool_size=(2, 2))\n",
    "    print layer4.output_shape \n",
    "    \n",
    "    layer4 = lasagne.layers.flatten(layer4)\n",
    "    print layer4.output_shape \n",
    "    \n",
    "    layer5 = lasagne.layers.DenseLayer(layer4,num_units=120,nonlinearity=lasagne.nonlinearities.rectify)    \n",
    "    print layer5.output_shape \n",
    "    \n",
    "    layer6 = lasagne.layers.DenseLayer(layer5,num_units=84,nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    print layer6.output_shape \n",
    "    \n",
    "    outputlayer = lasagne.layers.DenseLayer(layer6,num_units=10,nonlinearity=lasagne.nonlinearities.softmax)     \n",
    "    print outputlayer.output_shape \n",
    "    \n",
    "    return layer1, layer2, layer3, layer4, layer5, layer6, outputlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = theano.tensor.tensor4()\n",
    "Y = theano.tensor.matrix()\n",
    "layer1, layer2, layer3, layer4, layer5, layer6, outputlayer = buildLeNet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>6) Training function</h4>\n",
    "Define the functions for training the network. We will use negative log likelihood (called categorical cross-entropy in lasagne) as loss function (second row) and stochastic gradient descent with momentum as optimiser (fourth row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputtrain = lasagne.layers.get_output(outputlayer) #function that gets the output from the network defined before.\n",
    "trainloss = lasagne.objectives.categorical_crossentropy(outputtrain, Y).mean() #function that computes the mean crossentropy between the output and the real labels.\n",
    "params = lasagne.layers.get_all_params(outputlayer, trainable=True) #function that gets all the parameters (weights) in the network.\n",
    "updates = lasagne.updates.momentum(trainloss, params, learning_rate=0.001) #function that performs an update of the weights based on the loss.\n",
    "train = theano.function(inputs=[X, Y], outputs=trainloss, updates=updates, allow_input_downcast=True) #function that does all the above based on training samples X and real labels Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>7) Validation function</h4>\n",
    "Define a function to validate the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate = theano.function(inputs=[X, Y], outputs=trainloss, allow_input_downcast=True) #function that computes the loss without performing an update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>8) Test function</h4>\n",
    "Define the functions for testing the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputtest = lasagne.layers.get_output(outputlayer, deterministic=True) #function that gets the output from the network defined before.\n",
    "test = theano.function(inputs=[X], outputs=outputtest, allow_input_downcast=True) #function that gets the output based on input X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>9) Training the network</h4>\n",
    "Do the training in random batches of a specific number of samples (we set the values below to 250 batches of 100 samples). \n",
    "\n",
    "Use random.sample(a,n) to select a random batch of n samples from array a. \n",
    "\n",
    "Next, use the train(X,Y) function we have defined in 6) to perform an update of the network based on a random batch of training images X and training labels Y. \n",
    "\n",
    "The train function returns the loss. Save the loss of each training batch in the variable 'losslist' so we can look at them later (you can use .append() to add the current loss to the list).\n",
    "\n",
    "Also keep track of the loss for random batches from the validation set to see if your network is not overfitting on the training set. You can use validate(X,Y) from 7) to compute the loss on the validation set (without doing an update).\n",
    "\n",
    "<b>Remember</b> that if you restart the training process from the beginning you also need to reinitialise the network by running the cells starting from 5) again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingsamples = np.arange(len(train_set_labels)) #numbers from 0 until the number of samples\n",
    "validsamples = np.arange(len(valid_set_labels))\n",
    "\n",
    "minibatches = 1000\n",
    "minibatchsize = 100 \n",
    "\n",
    "losslist = []\n",
    "validlosslist = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in xrange(minibatches):\n",
    "    #select random training en validation samples and perform training and validation steps here.\n",
    "    \n",
    "    #solution:\n",
    "    #minibatchsamples = random.sample(trainingsamples,minibatchsize)                \n",
    "\n",
    "    #loss = train(train_set_images[minibatchsamples],labels[minibatchsamples])\n",
    "    #if (i+1)%50==0:\n",
    "    #    print 'Loss minibatch {}: {}'.format(i,loss)\n",
    "    #losslist.append(loss)            \n",
    "\n",
    "    #validminibatchsamples = random.sample(validsamples,minibatchsize)           \n",
    "    #validloss = validate(valid_set_images[validminibatchsamples],validlabels[validminibatchsamples])\n",
    "    #if (i+1)%50==0:\n",
    "    #    print 'Loss validation minibatch {}: {}'.format(i,validloss)\n",
    "    #validlosslist.append(validloss)\n",
    "\n",
    "t1 = time.time()\n",
    "print 'Training time: {} seconds'.format(t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>9) Loss curves</h4>\n",
    "Plot the loss curves for the training and validation sets (use plt.plot(losslist) for the training loss). \n",
    "\n",
    "Is 250 batches enough to train the network? How many do we need? \n",
    "\n",
    "What happens if you change the learning rate in 6)? \n",
    "\n",
    "What happens if you change the minibatchsize? \n",
    "\n",
    "What happens if you use another optimizer? \n",
    "\n",
    "Try to get the loss as low as possible! \n",
    "\n",
    "What happens if you make changes to the network? Use for example more or less filters or nodes, remove a layer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "#plt.plot(losslist)\n",
    "#plt.plot(validlosslist)\n",
    "#plt.legend(['Training loss','Validation loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>10) Evaluation on the test set</h4>\n",
    "Evaluate the network on the test set with the test(X) function we have defined in 8). You can use np.argmax() to select the node with the highest probability. \n",
    "\n",
    "How well did it do? How many of the 10 000 test samples did it label correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#t0 = time.time()\n",
    "#test_set_predictions = np.argmax(test(test_set_images),axis=1)\n",
    "#t1 = time.time()\n",
    "#print 'Testing time: {} seconds'.format(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TP = np.sum(test_set_labels==test_set_predictions)\n",
    "#print 'Accuracy: {}'.format(float(TP)/float(len(test_set_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>11) Visualising what the network has learned</h4>\n",
    "To see what is happening within the network we can visualise the learned filters and their feature maps. We now define an additional function that obtains the feature maps after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputlayer1 = lasagne.layers.get_output(layer1) \n",
    "outputfeatures = theano.function(inputs=[X], outputs=outputlayer1, allow_input_downcast=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>12) Visualising the features</h4>\n",
    "Let's look at the feature maps after the first layer for one of the images from the test set. We have defined the function 'outputfeatures' for that. \n",
    "\n",
    "Look at the shape of the features variable. \n",
    "\n",
    "Visualise the 6 features maps for some of the 10 000 test samples with plt.imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = outputfeatures(test_set_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print features.shape\n",
    "#for i in xrange(6):\n",
    "#    plt.figure()\n",
    "#    plt.imshow(features[1,i],cmap='gray_r',interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>13) Visualising the filters</h4>\n",
    "Let's look at the filters that are learned. We can use the lasagne function 'get_all_param_values' for that. These are the filters that are applied to the images to obtain the feature maps that we saw above.\n",
    "\n",
    "Look at the shape of the filters and biases.\n",
    "\n",
    "Visualise the 6 filters of the first layer with plt.imshow. Do you see any structure in the learned filters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = lasagne.layers.get_all_param_values(layer1)\n",
    "filters = weights[0]\n",
    "biases = weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print filters.shape\n",
    "#print biases.shape\n",
    "#for i in xrange(6):\n",
    "#    plt.figure()\n",
    "#    plt.imshow(filters[i,0],cmap='gray_r',interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>14) Visualising other layers </h4>\n",
    "Can you also visualise features and kernels of other layers? Take for example a look at the features after the third layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#outputlayer3 = lasagne.layers.get_output(layer3) \n",
    "#outputfeatures3 = theano.function(inputs=[X], outputs=outputlayer3, allow_input_downcast=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#features = outputfeatures3(test_set_images)\n",
    "#print features.shape\n",
    "#for i in xrange(6):\n",
    "#    plt.figure()\n",
    "#    plt.imshow(features[1,i],cmap='gray_r',interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#weights3 = lasagne.layers.get_all_param_values(layer3)\n",
    "#filters3 = weights3[0]\n",
    "#biases3 = weights3[1]\n",
    "#for i in xrange(6):\n",
    "#    plt.figure()\n",
    "#    plt.imshow(filters3[i,0],cmap='gray_r',interpolation='none')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
