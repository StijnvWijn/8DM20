{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MNIST digit recognition with LeNet</b>\n",
    "\n",
    "In this practical session we will build a convolutional neural network that is able to recognise the digits 0-9 from an image.\n",
    "\n",
    "You can run the code in a cell by selecting the cell and pressing Shift+Enter.\n",
    "\n",
    "1) First, import some of the packages we will need:\n",
    "\n",
    "Documentation for each of these packages can be found online: <br>\n",
    "For numpy: https://docs.scipy.org/doc/numpy-dev/user/quickstart.html <br>\n",
    "For matplotlib: http://matplotlib.org/api/pyplot_api.html <br>\n",
    "For lasagne: http://lasagne.readthedocs.io <br>\n",
    "For random: https://docs.python.org/2/library/random.html <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import theano\n",
    "import lasagne\n",
    "import time\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Load the data. Download the data from: http://deeplearning.net/data/mnist/mnist.pkl.gz and save it somewhere on your disc, change the path in the second cell below to the location where you have saved it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadMNIST(path):\n",
    "    f = gzip.open(path, 'rb')\n",
    "    train_set, valid_set, test_set = cPickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    train_set_labels = train_set[1]\n",
    "    train_set_images = np.resize(train_set[0],(len(train_set_labels),1,28,28))\n",
    "    train_set_images = np.pad(train_set_images,((0,0),(0,0),(2,2),(2,2)),'constant', constant_values=0)\n",
    "   \n",
    "    valid_set_labels = valid_set[1]\n",
    "    valid_set_images = np.resize(valid_set[0],(len(valid_set_labels),1,28,28))\n",
    "    valid_set_images = np.pad(valid_set_images,((0,0),(0,0),(2,2),(2,2)),'constant', constant_values=0)\n",
    "\n",
    "    test_set_labels = test_set[1]\n",
    "    test_set_images = np.resize(test_set[0],(len(test_set_labels),1,28,28))\n",
    "    test_set_images = np.pad(test_set_images,((0,0),(0,0),(2,2),(2,2)),'constant', constant_values=0)\n",
    "    \n",
    "    return train_set_labels, train_set_images, valid_set_labels, valid_set_images, test_set_labels, test_set_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set_labels, train_set_images, valid_set_labels, valid_set_images, test_set_labels, test_set_images = loadMNIST(r'D:\\mnist\\mnist.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Now let's look at the data we've just loaded! How many samples are in each set? (Use .shape to see the dimensions) How large are the images? How many samples per class? Show some of the images with plt.imshow (use cmap='gray_r' for black digits on a white background and interpolation='none' to see the real pixels), you can access one of the training images as: train_set_images[i,0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.imshow(train_set_images[0,0],cmap='gray_r',interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Convert the labels from a number between 0 and 9 to 'one-hot-encoding'. This means that for a label with number 3, there should be a 1 at element 3 and 0 everywhere else, i.e. [0, 0, 0, 1, 0, 0, 0, 0 ,0 ,0]. These are our target nodes, the node at position 3 should be active when the input image shows a 3. The code below does this for the training labels. \n",
    "\n",
    "Do the same for the validation labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = np.zeros((len(train_set_labels),10),dtype=np.int16)        \n",
    "for n in xrange(10):\n",
    "    labels[:,n] = train_set_labels==n\n",
    "\n",
    "print train_set_labels[:10]\n",
    "print labels[:10]\n",
    "\n",
    "#validlabels = np.zeros((len(valid_set_labels),10),dtype=np.int16)        \n",
    "#for n in xrange(10):\n",
    "#    validlabels[:,n] = valid_set_labels==n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Build the LeNet network. The print network.output_shape statement shows the dimensions after the current layer. Can you recognise all the elements of the network from the lecture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildLeNet(X1):\n",
    "    network = lasagne.layers.InputLayer(shape=(None, 1, 32, 32),input_var=X1)    \n",
    "    print network.output_shape     \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=6, filter_size=(5,5), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    print network.output_shape \n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    print network.output_shape \n",
    "    network = lasagne.layers.Conv2DLayer(network, num_filters=16, filter_size=(5,5), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    print network.output_shape \n",
    "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
    "    print network.output_shape \n",
    "    network = lasagne.layers.flatten(network)\n",
    "    print network.output_shape \n",
    "    network = lasagne.layers.DenseLayer(network,num_units=120,nonlinearity=lasagne.nonlinearities.rectify)    \n",
    "    print network.output_shape \n",
    "    network = lasagne.layers.DenseLayer(network,num_units=84,nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    print network.output_shape \n",
    "    network = lasagne.layers.DenseLayer(network,num_units=10,nonlinearity=lasagne.nonlinearities.softmax)     \n",
    "    print network.output_shape \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = theano.tensor.tensor4()\n",
    "Y = theano.tensor.matrix()\n",
    "network = buildLeNet(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Define the functions for training the network. We will use categorical cross-entropy as loss function (second row) and stochastic gradient descent with momentum as optimiser (fourth row)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputtrain = lasagne.layers.get_output(network) #function that gets the output from the network defined before.\n",
    "trainloss = lasagne.objectives.categorical_crossentropy(outputtrain, Y).mean() #function that computes the mean crossentropy between the output and the real labels.\n",
    "params = lasagne.layers.get_all_params(network, trainable=True) #function that gets all the parameters (weights) in the network.\n",
    "updates = lasagne.updates.momentum(trainloss, params, learning_rate=0.001) #function that performs an update of the weights based on the loss.\n",
    "train = theano.function(inputs=[X, Y], outputs=trainloss, updates=updates, allow_input_downcast=True) #function that does all the above based on training samples X and real labels Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Define a function to validate the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate = theano.function(inputs=[X, Y], outputs=trainloss, allow_input_downcast=True) #function that computes the loss without performing an update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Define the functions for testing the network. deterministic=True turns off dropout in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputtest = lasagne.layers.get_output(network, deterministic=True) #function that gets the output from the network defined before.\n",
    "test = theano.function(inputs=[X], outputs=outputtest, allow_input_downcast=True) #function that gets the output based on input X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Do the training in random batches of a specific number of samples (we set the values below to 250 batches of 100 samples). You can use random.sample(a,n) to select n samples from array a. \n",
    "\n",
    "You can use the train(X,Y) function we have defined in 6) to perform an update of the network based on training images X and training labels Y, this function returns the loss. Save the loss of each training batch in the variable 'losslist' so we can look at them later (you can use .append()).\n",
    "\n",
    "Also keep track of the loss for random batches from the validation set to see if your network is not overfitting (you can use validate(X,Y) from 7)).\n",
    "\n",
    "<b>Remember</b> that if you restart the training process from the beginning you also need to reinitialise the network by running the cells starting from 5) again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainingsamples = np.arange(len(train_set_labels)) #numbers from 0 until the number of samples\n",
    "validsamples = np.arange(len(valid_set_labels))\n",
    "\n",
    "minibatches = 250\n",
    "minibatchsize = 100 \n",
    "\n",
    "losslist = []\n",
    "validlosslist = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in xrange(minibatches):\n",
    "    #select random training en validation samples and perform training and validation steps here.\n",
    "    \n",
    "    #solution:\n",
    "    #minibatchsamples = random.sample(trainingsamples,minibatchsize)                \n",
    "\n",
    "    #loss = train(train_set_images[minibatchsamples],labels[minibatchsamples])\n",
    "    #if (i+1)%50==0:\n",
    "    #    print 'Loss minibatch {}: {}'.format(i,loss)\n",
    "    #losslist.append(loss)            \n",
    "\n",
    "    #validminibatchsamples = random.sample(validsamples,minibatchsize)           \n",
    "    #validloss = validate(valid_set_images[validminibatchsamples],validlabels[validminibatchsamples])\n",
    "    #if (i+1)%50==0:\n",
    "    #    print 'Loss validation minibatch {}: {}'.format(i,validloss)\n",
    "    #validlosslist.append(validloss)\n",
    "\n",
    "t1 = time.time()\n",
    "print 'Training time: {} seconds'.format(t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Plot the loss curves for the training and validation sets (use plt.plot). Is 250 batches enough to train the network? How many do we need? What happens if you change the learning rate in 6)? What happens if you change the minibatchsize? What happens if you use another optimizer? Try to get the loss as low as possible! What happens if you make changes to the network? Use for example more or less filters or nodes, remove a layer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.figure()\n",
    "#plt.plot(losslist)\n",
    "#plt.plot(validlosslist)\n",
    "#plt.legend(['Training loss','Validation loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) Evaluate the network on the test set with the test(X) function we have defined in 8). You can use np.argmax() to select the node with the highest probability. How well did it do? How many of the 10 000 test samples did it label correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#t0 = time.time()\n",
    "#test_set_predictions = np.argmax(test(test_set_images),axis=1)\n",
    "#t1 = time.time()\n",
    "#print 'Testing time: {} seconds'.format(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TP = np.sum(test_set_labels==test_set_predictions)\n",
    "#print 'Accuracy: {}'.format(float(TP)/float(len(test_set_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) To see what is happening we can visualise the learned filters and their feature maps. \n",
    "\n",
    "First let's define the network again, but this time also return the result after the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildLeNet(X1):\n",
    "    network1 = lasagne.layers.InputLayer(shape=(None, 1, 32, 32),input_var=X1)  \n",
    "    network1 = lasagne.layers.Conv2DLayer(network1, num_filters=6, filter_size=(5,5), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    network2 = lasagne.layers.MaxPool2DLayer(network1, pool_size=(2, 2))\n",
    "    network2 = lasagne.layers.Conv2DLayer(network2, num_filters=16, filter_size=(5,5), nonlinearity=lasagne.nonlinearities.rectify, W=lasagne.init.GlorotUniform())\n",
    "    network2 = lasagne.layers.MaxPool2DLayer(network2, pool_size=(2, 2))\n",
    "    network2 = lasagne.layers.flatten(network2)\n",
    "    network2 = lasagne.layers.DenseLayer(network2,num_units=120,nonlinearity=lasagne.nonlinearities.rectify)    \n",
    "    network2 = lasagne.layers.DenseLayer(network2,num_units=84,nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    network2 = lasagne.layers.DenseLayer(network2,num_units=10,nonlinearity=lasagne.nonlinearities.softmax)      \n",
    "    return network1, network2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = theano.tensor.tensor4()\n",
    "Y = theano.tensor.matrix()\n",
    "network1, network2 = buildLeNet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputtrain = lasagne.layers.get_output(network2) \n",
    "trainloss = lasagne.objectives.categorical_crossentropy(outputtrain, Y).mean() \n",
    "params = lasagne.layers.get_all_params(network2, trainable=True) \n",
    "updates = lasagne.updates.momentum(trainloss, params, learning_rate=0.001) \n",
    "train = theano.function(inputs=[X, Y], outputs=trainloss, updates=updates, allow_input_downcast=True) \n",
    "\n",
    "validate = theano.function(inputs=[X, Y], outputs=trainloss, allow_input_downcast=True)\n",
    "\n",
    "outputtest = lasagne.layers.get_output(network2, deterministic=True) \n",
    "test = theano.function(inputs=[X], outputs=outputtest, allow_input_downcast=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define an additional function that obtains the feature maps after the first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputlayer1 = lasagne.layers.get_output(network1) \n",
    "outputfeatures = theano.function(inputs=[X], outputs=outputlayer1, allow_input_downcast=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy your training code below and retrain the network in the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trainingsamples = np.arange(len(train_set_labels)) #numbers from 0 until the number of samples\n",
    "#validsamples = np.arange(len(valid_set_labels))\n",
    "\n",
    "#minibatches = 1000\n",
    "#minibatchsize = 100 \n",
    "\n",
    "#losslist = []\n",
    "#validlosslist = []\n",
    "\n",
    "#t0 = time.time()\n",
    "\n",
    "#for i in xrange(minibatches):\n",
    "    #select random training en validation samples and perform training and validation steps here.\n",
    "    \n",
    "    #solution:\n",
    "    #minibatchsamples = random.sample(trainingsamples,minibatchsize)                \n",
    "    \n",
    "    #loss = train(train_set_images[minibatchsamples],labels[minibatchsamples])\n",
    "    #if (i+1)%50==0:\n",
    "    #    print 'Loss minibatch {}: {}'.format(i,loss)\n",
    "    #losslist.append(loss)            \n",
    "\n",
    "    #validminibatchsamples = random.sample(validsamples,minibatchsize)           \n",
    "    #validloss = validate(valid_set_images[validminibatchsamples],validlabels[validminibatchsamples])\n",
    "    #if (i+1)%50==0:\n",
    "    #    print 'Loss validation minibatch {}: {}'.format(i,validloss)\n",
    "    #validlosslist.append(validloss)\n",
    "\n",
    "#t1 = time.time()\n",
    "#print 'Training time: {} seconds'.format(t1-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the feature maps after the first layer for one of the images from the test set. We have defined the function 'outputfeatures' for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = outputfeatures(test_set_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(6):\n",
    "    plt.figure()\n",
    "    plt.imshow(features[1,i],cmap='gray_r',interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the filters that are learned. We can use the lasagne function 'get_all_param_values' for that. These are the filters that are applied to the images to obtain the feature maps that we saw above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = lasagne.layers.get_all_param_values(network1)\n",
    "filters = weights[0]\n",
    "biases = weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print filters.shape\n",
    "print biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(6):\n",
    "    plt.figure()\n",
    "    plt.imshow(filters[i,0],cmap='gray_r',interpolation='none')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
